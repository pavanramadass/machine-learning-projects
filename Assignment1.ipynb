{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "18uA_QVZCXEKolSe3neTKB_0RCPRy4oDr",
      "authorship_tag": "ABX9TyO8nRffu2ZAUMV672sxRkc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanramadass/machine-learning-projects/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG_n8qgE6Zm4"
      },
      "source": [
        "TASK 1:\n",
        "\n",
        "The machine learning problem I would like to solve using logistic regression is classifying whether someone has diabetes or not.\n",
        "Logistic regression is the best choice for this problem, because logistic regression's output is a binary output, in other words,\n",
        "the output I receive from logistic regression is a yes or no. Therefore, logistic regression will classify whether someone has \n",
        "diabetes based on the inputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g_tuw_3_RAb"
      },
      "source": [
        "TASK 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZOanM3Y_SvI"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "data = pd.read_csv(io.BytesIO(uploaded['diabetes2.csv']))\n",
        "\n",
        "'''\n",
        "Link to Dataset:\n",
        "https://www.kaggle.com/kandij/diabetes-dataset\n",
        "'''\n",
        "\n",
        "'''\n",
        "For my Exploratory Data Analysis, I chose to check for NaNs and also outliers. I chose\n",
        "to check for these two features in the data set, because these are most important to understand\n",
        "when it comes to training a machine learning algorithm. \n",
        "If there are NaNs when there should not be, it will negatively affect the training as well as \n",
        "if there are many outliers. Moreover, having may NaNs as well as outliers also shows the \n",
        "dataset may be a faulty/unreliable dataset for training an AI on. \n",
        "'''\n",
        "\n",
        "'''\n",
        "I am using the .info() command to find the range-index, the number of columns\n",
        "and their labels in the data, as well as how many non-null/null values are there\n",
        "in the dataset. \n",
        "\n",
        "Based on the .info() command, there are no null values which means this is a \n",
        "good dataset to work with.  \n",
        "'''\n",
        "data.info()\n",
        "\n",
        "'''\n",
        "I am using seaborn pairplot to analyze for outliers in the dataset. From the plots, there\n",
        "are no major outliers in the dataset. However, for skin-thickness, insulin, and bmi there\n",
        "were a few outliers, so I am curious to see how these outliers will affect the performance\n",
        "of the Logistic Regression. Since out of the many normal data points, only a very very small\n",
        "amount are outliers, I chose not to remove them as I hypothesis them to not have a major adverse affect\n",
        "in the training.  \n",
        "'''\n",
        "sns.pairplot(data, height=2.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4cKNca06iDh"
      },
      "source": [
        "TASK 3 & TASK 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeOPFB4cd3-r"
      },
      "source": [
        "# Importing Libraries\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import math \n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "def sigmoid(w, X, b):\n",
        "  dot_product = np.dot(X, w.T) + b \n",
        "  sig = 1/(1 + np.exp(-dot_product))  \n",
        "  return sig\n",
        "\n",
        "# used to compare the gradient descents and their optimizers \n",
        "def cost_function(n, y_actual, y_pred):\n",
        "  return (-1/n) * np.sum(y_actual * np.log(y_pred) + (1 - y_actual) * np.log(1 - y_pred))\n",
        "\n",
        "# Batch gradient descent calculation \n",
        "def gradient_descent_batch(w, b, X, Y):\n",
        "  n = X.shape[0]\n",
        "  A = sigmoid(w, X, b)\n",
        "  cost = cost_function(n, Y, A)\n",
        "  dw = np.dot((A - Y).T, X) / n \n",
        "  db = np.sum(A - Y) / n \n",
        "  gradients = {\"dw\": dw, \"db\": db} \n",
        "  return gradients, cost\n",
        "\n",
        "# Stoch gradient descent calculation \n",
        "def gradient_descent_stoch(w, b, X, Y):\n",
        "  n = X.shape[0]\n",
        "  A = sigmoid(w, X, b)\n",
        "  cost = cost_function(n, Y, A)\n",
        "  dw = ((A - Y).T * X) / n \n",
        "  db = np.sum(A - Y) / n\n",
        "  gradients = {\"dw\": dw, \"db\": db}\n",
        "  return gradients, cost\n",
        "\n",
        "def update(w, b, X, Y, data_list, n, b1, b2, e, epoch, gradient_type, optimizer_type):\n",
        "  costs = []\n",
        "  if gradient_type == 1: # batch gradient\n",
        "    if optimizer_type == 0: # no optimizer\n",
        "      for i in range(epoch):\n",
        "        gradients, cost = gradient_descent_batch(w, b, X, Y) \n",
        "        dw = gradients[\"dw\"]\n",
        "        db = gradients[\"db\"]\n",
        "        w = w - (n * dw)\n",
        "        b = b - (n * db) \n",
        "        costs.append(cost)\n",
        "      coeffs = {\"w\": w, \"b\": b}\n",
        "      gradient = {\"dw\": dw, \"db\": db}\n",
        "      return coeffs, gradient, costs\n",
        "    elif optimizer_type == 1: # adam optimizer\n",
        "      m_w, v_w, m_b, v_b, t = 0, 0, 0, 0, 0  \n",
        "      for i in range(epoch):\n",
        "        gradients, cost = gradient_descent_batch(w, b, X, Y)\n",
        "        dw = gradients[\"dw\"]\n",
        "        db = gradients[\"db\"]\n",
        "        t += 1\n",
        "        w, b, m_w, v_w, m_b, v_b = adam(w, b, dw, db, m_w, v_w, m_b, v_b, n, b1, b2, e, t) \n",
        "        costs.append(cost)\n",
        "      coeffs = {\"w\": w, \"b\": b}\n",
        "      gradient = {\"dw\": dw, \"db\": db}\n",
        "      return coeffs, gradient, costs\n",
        "    elif optimizer_type == 2: # nadam optimizer\n",
        "      m_w, v_w, m_b, v_b, t = 0, 0, 0, 0, 0\n",
        "      for i in range(epoch):\n",
        "        gradients, cost = gradient_descent_batch(w, b, X, Y)\n",
        "        dw = gradients[\"dw\"]\n",
        "        db = gradients[\"db\"]\n",
        "        t += 1\n",
        "        w, b, m_w, v_w, m_b, v_b = nadam(w, b, dw, db, m_w, v_w, m_b, v_b, n, b1, b2, e, t) \n",
        "        costs.append(cost)\n",
        "      coeffs = {\"w\": w, \"b\": b}\n",
        "      gradient = {\"dw\": dw, \"db\": db}\n",
        "      return coeffs, gradient, costs\n",
        "  elif gradient_type == 2: # stoch gradient\n",
        "    if optimizer_type == 0: # no optimizer\n",
        "      for i in range(epoch):\n",
        "        np.random.shuffle(data_list)\n",
        "        for example in data_list:\n",
        "          X = example[0:-1]\n",
        "          Y = example[-1] \n",
        "          gradients, cost = gradient_descent_stoch(w, b, X, Y) \n",
        "          dw = gradients[\"dw\"]\n",
        "          db = gradients[\"db\"]\n",
        "          w = w - (n * dw)\n",
        "          b = b - (n * db)\n",
        "          costs.append(cost)\n",
        "      coeffs = {\"w\": w, \"b\": b}\n",
        "      gradient = {\"dw\": dw, \"db\": db}\n",
        "      return coeffs, gradient, costs \n",
        "    elif optimizer_type == 1: # adam optimizer\n",
        "      for i in range(epoch):\n",
        "        np.random.shuffle(data_list)\n",
        "        m_w, v_w, m_b, v_b, t = 0, 0, 0, 0, 0\n",
        "        for example in data_list:\n",
        "          X = example[0:-1]\n",
        "          Y = example[-1]\n",
        "          gradients, cost = gradient_descent_stoch(w, b, X, Y)\n",
        "          dw = gradients[\"dw\"]\n",
        "          db = gradients[\"db\"]\n",
        "          w, b, m_w, v_w, m_b, v_b = adam(w, b, dw, db, m_w, v_w, m_b, v_b, n, b1, b2, e, t)\n",
        "          costs.append(cost)\n",
        "      coeffs = {\"w\": w, \"b\": b}\n",
        "      gradient = {\"dw\": dw, \"db\": db}\n",
        "      return coeffs, gradient, costs\n",
        "    elif optimizer_type == 2: # nadam optimizer \n",
        "      for i in range(epoch):\n",
        "        np.random.shuffle(data_list)\n",
        "        m_w, v_w, m_b, v_b, t = 0, 0, 0, 0, 0\n",
        "        for example in data_list:\n",
        "          X = example[0:-1]\n",
        "          Y = example[-1]\n",
        "          gradients, cost = gradient_descent_stoch(w, b, X, Y)\n",
        "          dw = gradients[\"dw\"]\n",
        "          db = gradients[\"db\"]\n",
        "          w, b, m_w, v_w, m_b, v_b= nadam(w, b, dw, db, m_w, v_w, m_b, v_b, n, b1, b2, e, t)\n",
        "          costs.append(cost)\n",
        "      coeffs = {\"w\": w, \"b\": b}\n",
        "      gradient = {\"dw\": dw, \"db\": db}\n",
        "      return coeffs, gradient, costs\n",
        "        \n",
        "def adam(w, b, dw, db, m_w, v_w, m_b, v_b, n, b1, b2, e, t):\n",
        "  m_w = b1 * m_w + (1 - b1) * dw \n",
        "  v_w = b2 * v_w + (1 - b2) * dw ** 2\n",
        "\n",
        "  m_b = b1 * m_b + (1 - b1) * db\n",
        "  v_b = b2 * v_b + (1 - b2) * db\n",
        "\n",
        "  m_w_ = m_w / (1 - b1 ** t) \n",
        "  v_w_ = v_w / (1 - b2 ** t)\n",
        "\n",
        "  m_b_ = m_b / (1 - b1 ** t)\n",
        "  v_b_ = v_b / (1 - b2 ** t) \n",
        "\n",
        "  w = w - (n / (np.sqrt(v_w_) + e)) * m_w_ \n",
        "  b = b - (n / (np.sqrt(v_b_) + e)) * m_b_ \n",
        "  return w, b, m_w, v_w, m_b, v_b \n",
        "\n",
        "\n",
        "def nadam(w, b, dw, db, m_w, v_w, m_b, v_b, n, b1, b2, e, t):\n",
        "  m_w = b1 * m_w + (1 - b1) * dw \n",
        "  m_b = b1 * m_b + (1 - b1) * db\n",
        "\n",
        "  m_w_ = m_w / (1 - b1 ** t)\n",
        "  m_b_ = m_b / (1 - b1 ** t) \n",
        "\n",
        "  v_w = b2 * v_w + (1 - b2) * (dw ** 2) \n",
        "  v_b = b2 * v_b + (1 - b2) * (db ** 2) \n",
        "\n",
        "  v_w_ = v_w / (1 - b2 ** t) \n",
        "  v_b_ = v_b / (1 - b2 ** t) \n",
        "\n",
        "  w = w - (n / (np.sqrt(v_w_) + e)) * (b1 * m_w_ + ((1 - b1) * dw) / (1 - b1 ** t))\n",
        "  b = b - (n / (np.sqrt(v_b_) + e)) * (b1 * m_b_ + ((1 - b1) * db) / (1 - b1 ** t))\n",
        "\n",
        "  return w, b, m_w, v_w, m_b, v_b \n",
        "\n",
        "def predict(y, y_size, x_size):\n",
        "  y_pred = np.zeros((1, x_size))\n",
        "  for i in range(y_size):\n",
        "    if y[0][i] > 0.5:\n",
        "      y_pred[0][i] = 1\n",
        "  return y_pred \n",
        "\n",
        "\n",
        "#Data preprocessing\n",
        "uploaded = files.upload()\n",
        "data = pd.read_csv(io.BytesIO(uploaded['diabetes2.csv']))\n",
        "\n",
        "costs = [[]]\n",
        "\n",
        "data_list = data.iloc[:, :].values \n",
        "\n",
        "X = data.iloc[:,0:-1].values # need to figure out which is for rows and which is for columns \n",
        "Y = data.iloc[:,-1].values\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)\n",
        "\n",
        "feature_size = X_train.shape[1]\n",
        "\n",
        "# Batch Gradient with no optimizer \n",
        "w = np.zeros((1, feature_size))\n",
        "b = 0\n",
        "\n",
        "coeff, gradient, cost = update(w, b, X_train, Y_train, data_list, n=0.002, b1=0.9, b2=0.999, e=0.00000001, epoch=1000, gradient_type=1, optimizer_type=0)\n",
        "costs.append(cost)\n",
        "\n",
        "w = coeff[\"w\"]\n",
        "b = coeff[\"b\"]\n",
        "y = sigmoid(w, X, b)\n",
        "x_size = X_train.shape[0]\n",
        "y_size = y.shape[1] \n",
        "y_pred = predict(y, y_size, x_size) \n",
        "\n",
        "\n",
        "# Batch Gradient with adam optimizer\n",
        "w = np.zeros((1, feature_size))\n",
        "b = 0\n",
        "\n",
        "coeff, gradient, cost = update(w, b, X_train, Y_train, data_list, n=0.002, b1=0.9, b2=0.999, e=0.00000001, epoch=1000, gradient_type=1, optimizer_type=1)\n",
        "costs.append(cost)\n",
        "w = coeff[\"w\"]\n",
        "b = coeff[\"b\"]\n",
        "y = sigmoid(w, X, b)\n",
        "x_size = X_train.shape[0]\n",
        "y_size = y.shape[1] \n",
        "y_pred = predict(y, y_size, x_size) \n",
        "\n",
        "# Batch Gradient with nadam optimizer\n",
        "w = np.zeros((1, feature_size))\n",
        "b = 0\n",
        "\n",
        "coeff, gradient, cost = update(w, b, X_train, Y_train, data_list, n=0.002, b1=0.9, b2=0.999, e=0.00000001, epoch=1000, gradient_type=1, optimizer_type=2)\n",
        "costs.append(cost)\n",
        "w = coeff[\"w\"]\n",
        "b = coeff[\"b\"]\n",
        "y = sigmoid(w, X, b)\n",
        "x_size = X_train.shape[0]\n",
        "y_size = y.shape[1] \n",
        "y_pred = predict(y, y_size, x_size) \n",
        "\n",
        "# Stochastic Gradient with no optimizer\n",
        "w = np.zeros((1, feature_size))\n",
        "b = 0\n",
        "\n",
        "coeff, gradient, cost = update(w, b, X_train, Y_train, data_list, n=0.002, b1=0.9, b2=0.999, e=0.00000001, epoch=1000, gradient_type=2, optimizer_type=0)\n",
        "costs.append(cost)\n",
        "w = coeff[\"w\"]\n",
        "b = coeff[\"b\"]\n",
        "y = sigmoid(w, X, b)\n",
        "x_size = X_train.shape[0]\n",
        "y_size = y.shape[1] \n",
        "y_pred = predict(y, y_size, x_size) \n",
        "\n",
        "# Stochastic Gradient with adam optimizer\n",
        "w = np.zeros((1, feature_size))\n",
        "b = 0\n",
        "\n",
        "coeff, gradient, cost = update(w, b, X_train, Y_train, data_list, n=0.002, b1=0.9, b2=0.999, e=0.00000001, epoch=1000, gradient_type=2, optimizer_type=1)\n",
        "costs.append(cost)\n",
        "w = coeff[\"w\"]\n",
        "b = coeff[\"b\"]\n",
        "y = sigmoid(w, X, b)\n",
        "x_size = X_train.shape[0]\n",
        "y_size = y.shape[1] \n",
        "y_pred = predict(y, y_size, x_size) \n",
        "\n",
        "# Stochastic Gradient with nadam optimizer\n",
        "w = np.zeros((1, feature_size))\n",
        "b = 0\n",
        "\n",
        "coeff, gradient, cost = update(w, b, X_train, Y_train, data_list, n=0.002, b1=0.9, b2=0.999, e=0.00000001, epoch=1000, gradient_type=2, optimizer_type=2)\n",
        "costs.append(cost)\n",
        "w = coeff[\"w\"]\n",
        "b = coeff[\"b\"]\n",
        "y = sigmoid(w, X, b)\n",
        "x_size = X_train.shape[0]\n",
        "y_size = y.shape[1] \n",
        "y_pred = predict(y, y_size, x_size) \n",
        "\n",
        "# Conclusive plotting \n",
        "plt.plot(costs[1], '-g', label='batch gradient--no optimizer')\n",
        "plt.plot(costs[2], '-r', label='batch gradient--adam')\n",
        "plt.plot(costs[3], '-b', label='batch gradient--nadam')\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('epochs')\n",
        "plt.title('Cost Analysis of batch gradient')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(costs[4], '-g', label='stoch gradient--no optimizer')\n",
        "plt.plot(costs[5],'-r', label='stoch gradient--adam')\n",
        "plt.plot(costs[6], '-b', label='stoch gradient--nadam')\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('epochs')\n",
        "plt.title('Cost Analysis of stochastic gradient')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Conclusive Writing\n",
        "'''\n",
        "In conclusion, I noticed that using the optimizers was better than using the vanilla gradients. Moreover, \n",
        "comparing the vanilla gradients, batch gradient descent had a better cost reduction compared to the \n",
        "stochastic gradient. Therefore, I say we must use optimizers, because they increase the performance of \n",
        "updating the coefficients w and b. The optimizers I used were adam and nadam, and both assisted in improving\n",
        "the updating performance by incorporating other parameters into the calculation such as betas and epsilon as\n",
        "well as biases. \n",
        "\n",
        "Now, when comparing nadam and adam, both optimizers seem to be on par with one another. This may be due to the\n",
        "data set I have chosen, because in the research paper it was discussed that nadam is a more optimal optimizer than\n",
        "adam because it incorporates NAG into the adam optimizer. Therefore, I hypothesize that nadam would work best on \n",
        "very large datasets such as data sets used for image recognition where the algorithm trains through thousands of images\n",
        "and, when it comes to a smaller data set like the one I have chosen, it does not matter whether one uses adam or nadam when updating \n",
        "the coefficients of the logistic function. \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}