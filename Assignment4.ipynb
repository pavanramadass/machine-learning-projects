{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMb8ID7nUpvmsLAcnX7daWQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavanramadass/machine-learning-projects/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F2s7egXXC1k"
      },
      "source": [
        "Task 1\n",
        "\n",
        "https://www.investing.com/currencies/eur-usd-historical-data\n",
        "\n",
        "Downloaded dataset from investing.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtDEtpcK2zqO"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, LSTM\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Data Preprocessing\n",
        "uploaded = files.upload()\n",
        "dataset = pd.read_csv(io.BytesIO(uploaded['eur_usd.csv']))\n",
        "\n",
        "filtered_data = dataset.filter(['Price'])\n",
        "data = filtered_data.values \n",
        "\n",
        "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
        "transformed_data = mmscaler.fit_transform(data)\n",
        "\n",
        "index = filtered_data.columns.get_loc(\"Price\")\n",
        "\n",
        "train_data_len = math.ceil(transformed_data.shape[0] * 0.7)\n",
        "train_data = transformed_data[0:train_data_len, :]\n",
        "test_data = transformed_data[train_data_len - 5:, :]\n",
        "  \n",
        "# Splitting dataset into train and test sets manually since the dataset is time dependent \n",
        "x_train, y_train = [], []\n",
        "for i in range(5, train_data_len):\n",
        "  x_train.append(data[i - 5:i, :])\n",
        "  y_train.append(data[i, index])\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test, y_test = [], []\n",
        "test_data_len = test_data.shape[0]\n",
        "for i in range(5, test_data_len):\n",
        "  x_test.append(data[i - 5:i, :])\n",
        "  y_test.append(data[i, index])\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Recurrent Neural Network Model\n",
        "model_rnn = Sequential()\n",
        "\n",
        "model_rnn.add(SimpleRNN(5, return_sequences=True, input_shape=(x_train.shape[1], 1))) \n",
        "model_rnn.add(SimpleRNN(5, return_sequences=False))\n",
        "model_rnn.add(Dense(25, activation='relu'))\n",
        "model_rnn.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model_rnn.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_rnn.fit(x_train, y_train, batch_size=16, epochs=25)\n",
        "\n",
        "# Testing\n",
        "preds_rnn = model_rnn.predict(x_test)\n",
        "\n",
        "mae = mean_absolute_error(preds_rnn, y_test)\n",
        "print('MAE: ' + str(round(mae, 5)))\n",
        "\n",
        "# LSTM Model\n",
        "model_lstm = Sequential()\n",
        "\n",
        "model_lstm.add(LSTM(5, return_sequences=True, input_shape=(x_train.shape[1], 1))) \n",
        "model_lstm.add(LSTM(5, return_sequences=False))\n",
        "model_lstm.add(Dense(25, activation='relu'))\n",
        "model_lstm.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_lstm.fit(x_train, y_train, batch_size=16, epochs=25)\n",
        "\n",
        "# Testing\n",
        "preds_lstm = model_lstm.predict(x_test)\n",
        "\n",
        "mae = mean_absolute_error(preds_lstm, y_test)\n",
        "print('MAE: ' + str(round(mae, 5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOI3EN2UT_X-"
      },
      "source": [
        "I chose to use the EUR/USD forex dataset to solve the problem of predicting future currency price per monthly timeframe. Unlike stocks, forex is more cyclical, therefore it is worth more in knowing what the future of currency market will be compared to the stock market. \n",
        "\n",
        "As for my neural network framework, I chose to use Keras due to it being well known by me. \n",
        "\n",
        "For my RNN, I used keras's SimpleRNN. The structure of my RNN I have two layers of the simpleRNN. Afterwards, I have one dense layer with the relu activation function, and the output dense layer.\n",
        "Lastly, the metric I used is mean absolute error. This metric just shows what the average error is for the model. So, a low mean average score means the model is better. \n",
        "\n",
        "After implemeting LSTM, the major difference I noticed was the mean average score. The mean average score for the LSTM was lower, thus proving that LSTM is a better model than RNN. The mean average score for the LSTM is 0.03081 and the mean average score for the RNN is 0.03328.\n",
        "\n",
        "The reason why LSTM had a better mean average score is because with LSTMs we are using more control. This in turn controls the flow and mixing of inputs to give us more control. So, by giving us more control, we are able to have less error in our predictions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZRRk0S6XGDN"
      },
      "source": [
        "Task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNOFsgGJKJrZ"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjE7UimNXH-5"
      },
      "source": [
        "NOTE:\n",
        "I was pretty unsure where to find a good dataset for nlp, so after much research I found gensim. Gensim has their own implementation of \n",
        "Word2Vec (I assume it is similar to the one you showed us in class), and they have many datasets that users can use for training the Word2Vec\n",
        "neural network. Overall I found gensim to be a useful and resourceful nlp/topic modelling toolkit or framework. \n",
        "\n",
        "So, I decided to use Gensim as my dataset as well as word2vec neural network. \n",
        "I implemented the cosine similarity as its own function, and for the dissimilarity I just did one minus the cosine similarity, because\n",
        "in from my understanding if two words are 80% similar, then they are 20% dissimilar. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_XVwnfS6NPb"
      },
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import math \n",
        "\n",
        "corpus = api.load('text8')\n",
        "\n",
        "model = Word2Vec(corpus) \n",
        "\n",
        "def cos_similarity_func(vec_word1, vec_word2):\n",
        "  numerator = 0\n",
        "  denominator = 0\n",
        "  part_1 = 0\n",
        "  part_2 = 0\n",
        "  for num1, num2 in zip(vec_word1, vec_word2):\n",
        "    numerator += (num1 * num2)\n",
        "\n",
        "  for num1, num2 in zip(vec_word1, vec_word2):\n",
        "    part_1 += num1 ** 2\n",
        "    part_2 += num2 ** 2\n",
        "\n",
        "  denominator = math.sqrt(part_1) * math.sqrt(part_2)\n",
        "\n",
        "  cos_similarity = numerator / denominator \n",
        "\n",
        "  return cos_similarity \n",
        "\n",
        "\n",
        "word1 = input(\"Enter first word: \")\n",
        "word2 = input(\"Enter second word: \")\n",
        "\n",
        "vec_word1 = model.wv[word1]\n",
        "vec_word2 = model.wv[word2]\n",
        "\n",
        "similarity = cos_similarity_func(vec_word1, vec_word2)\n",
        "dissimilarity = 1 - similarity\n",
        "\n",
        "print(\"Similarity\")\n",
        "print(similarity)\n",
        "\n",
        "print(\"Dissimilarity\")\n",
        "print(dissimilarity)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}